# ğŸš¦ Adversarial Attacks on Traffic Sign Recognition Using Diffusion Models

[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-ee4c2c.svg)](https://pytorch.org/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

This repository contains the implementation for **"Adversarial Attacks on Traffic Sign Recognition Using Diffusion Models"**, a research project investigating the vulnerability of traffic sign classification systems to realistic, semantic adversarial perturbations generated by Stable Diffusion.

## ğŸ“‹ Overview

Traditional adversarial attacks (FGSM, PGD, C&W) create imperceptible pixel-level noise that can fool deep neural networks. However, these perturbations are easily detectable and don't represent realistic threat scenarios. This project explores **diffusion-based adversarial attacks** that generate realistic weather and lighting conditions (rain, fog, snow, glare) to create more naturalistic adversarial examples.

### Key Contributions

- ğŸŒ§ï¸ **Weather-based adversarial attacks** using Stable Diffusion img2img pipeline
- ğŸ“Š **Comprehensive benchmark** comparing diffusion vs. classical attacks (FGSM, PGD, C&W)
- ğŸ”„ **Transferability analysis** across multiple architectures (ResNet-50, EfficientNet-B0, ViT)
- ğŸ“ **Imperceptibility metrics** (LPIPS, SSIM) for attack quality assessment

## ğŸ—ï¸ Project Structure

```
adversarial-traffic-signs/
â”œâ”€â”€ configs/
â”‚   â””â”€â”€ config.yaml          # Configuration file
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ config.py            # Configuration management
â”‚   â””â”€â”€ trainer.py           # Model training utilities
â”œâ”€â”€ data/
â”‚   â””â”€â”€ dataloader.py        # GTSRB dataset loaders
â”œâ”€â”€ models/
â”‚   â””â”€â”€ factory.py           # Model architectures (ResNet, EfficientNet, ViT)
â”œâ”€â”€ attacks/
â”‚   â”œâ”€â”€ classical.py         # FGSM, PGD, C&W attacks
â”‚   â””â”€â”€ diffusion.py         # Stable Diffusion attacks
â”œâ”€â”€ evaluation/
â”‚   â”œâ”€â”€ metrics.py           # LPIPS, SSIM, transferability
â”‚   â””â”€â”€ visualization.py     # Result plotting
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ train.py             # Training script
â”‚   â”œâ”€â”€ evaluate_attacks.py  # Attack evaluation
â”‚   â””â”€â”€ generate_diffusion_attacks.py
â”œâ”€â”€ notebooks/               # Jupyter notebooks
â”œâ”€â”€ results/                 # Output directory
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ setup.py
â””â”€â”€ README.md
```

## ğŸš€ Quick Start

### 1. Installation

```bash
# Clone the repository
git clone https://github.com/amitabhdas/adversarial-traffic-signs.git
cd adversarial-traffic-signs

# Create virtual environment
python -m venv venv
source venv/bin/activate  # Linux/Mac
# or
.\venv\Scripts\activate  # Windows

# Install dependencies
pip install -r requirements.txt

# Or install as package
pip install -e ".[all]"
```

### 2. Download GTSRB Dataset

Download the German Traffic Sign Recognition Benchmark (GTSRB) from:
- [Official GTSRB Website](https://benchmark.ini.rub.de/gtsrb_dataset.html)
- [Kaggle Mirror](https://www.kaggle.com/datasets/meowmeowmeowmeowmeow/gtsrb-german-traffic-sign)

Extract to `data/GTSRB/` with the following structure:
```
data/GTSRB/
â”œâ”€â”€ GTSRB_final_training_images/
â”‚   â”œâ”€â”€ 00000/
â”‚   â”œâ”€â”€ 00001/
â”‚   â””â”€â”€ ...
â”œâ”€â”€ GTSRB_final_test_images/
â””â”€â”€ GTSRB_Final_Test_GT/
    â””â”€â”€ GT-final_test.csv
```

### 3. Train Models

```bash
# Train all models (ResNet-50, EfficientNet-B0, ViT)
python scripts/train.py \
    --data_dir data/GTSRB \
    --models resnet50 efficientnet_b0 vit \
    --epochs 10 \
    --batch_size 64

# Train specific model
python scripts/train.py \
    --data_dir data/GTSRB \
    --models resnet50 \
    --epochs 20
```

### 4. Generate Diffusion Attacks

```bash
# Generate diffusion-based adversarial examples
python scripts/generate_diffusion_attacks.py \
    --data_dir data/GTSRB \
    --output_dir results/diffused_images \
    --num_samples 1000 \
    --strength 0.7
```

**Note:** Requires HuggingFace authentication for Stable Diffusion:
```python
from huggingface_hub import login
login("your_token_here")
```

### 5. Evaluate Attacks

```bash
# Evaluate all attacks
python scripts/evaluate_attacks.py \
    --data_dir data/GTSRB \
    --checkpoint_dir results/checkpoints \
    --diffusion_metadata results/diffused_images/metadata.csv \
    --output_dir results
```

## ğŸ“Š Results

### Attack Success Rate Comparison

| Attack     | ResNet-50 | EfficientNet-B0 | ViT   | Avg ASR |
|------------|-----------|-----------------|-------|---------|
| FGSM       | 45.2%     | 42.8%           | 48.1% | 45.4%   |
| PGD        | 68.3%     | 65.1%           | 71.2% | 68.2%   |
| C&W        | 95.1%     | 93.8%           | 94.5% | 94.5%   |
| **Diffusion** | **56.2%** | **57.3%**    | **57.1%** | **56.9%** |

### Imperceptibility Metrics

| Attack     | LPIPS (â†“) | SSIM (â†‘) |
|------------|-----------|----------|
| FGSM       | 0.012     | 0.982    |
| PGD        | 0.018     | 0.971    |
| C&W        | 0.008     | 0.989    |
| **Diffusion** | **0.245** | **0.421** |

**Key Insight:** Diffusion attacks have higher LPIPS/lower SSIM because they make semantic changes (adding weather effects), but these changes appear **natural and realistic** rather than as suspicious noise patterns.

## ğŸ”§ Configuration

Edit `configs/config.yaml` to customize:

```yaml
# Training settings
training:
  batch_size: 64
  num_epochs: 10
  learning_rate: 0.001

# Attack parameters
attacks:
  fgsm:
    eps: 0.03
  pgd:
    eps: 0.03
    alpha: 0.01
    steps: 10
  cw:
    c: 1
    steps: 100

# Diffusion settings
diffusion:
  model: "runwayml/stable-diffusion-v1-5"
  strength: 0.7
  guidance_scale: 7.5
```

## ğŸ“ Using with Google Colab

For GPU access, use Google Colab:

```python
# Mount Drive
from google.colab import drive
drive.mount('/content/drive')

# Clone repo
!git clone https://github.com/amitabhdas/adversarial-traffic-signs.git
%cd adversarial-traffic-signs

# Install dependencies
!pip install -r requirements.txt

# Run training
!python scripts/train.py --data_dir /content/GTSRB --epochs 10
```

## ğŸ§ª API Usage

```python
from models.factory import get_model, load_model
from attacks.classical import ClassicalAttacks
from attacks.diffusion import DiffusionAttacker
from evaluation.metrics import MetricsCalculator

# Load model
model = load_model('resnet50', 'checkpoints/resnet50_best.pth')

# Classical attacks
attacker = ClassicalAttacks(model, fgsm_eps=0.03)
adv_images = attacker.generate('FGSM', images, labels)

# Diffusion attacks
diff_attacker = DiffusionAttacker()
adv_image, prompt = diff_attacker.generate_single(image)

# Evaluate metrics
metrics = MetricsCalculator()
lpips_score = metrics.compute_lpips(original, adversarial)
ssim_score = metrics.compute_ssim(original, adversarial)
```

## ğŸ“– Citation

If you use this code in your research, please cite:

```bibtex
@misc{das2025adversarial,
  author = {Das, Amitabh},
  title = {Adversarial Attacks on Traffic Sign Recognition Using Diffusion Models},
  year = {2025},
  institution = {Northeastern University},
  advisor = {Elhamifar, Ehsan}
}
```

## ğŸ“œ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- **Advisor:** Prof. Ehsan Elhamifar, Northeastern University
- **Dataset:** German Traffic Sign Recognition Benchmark (GTSRB)
- **Models:** HuggingFace Transformers, timm
- **Diffusion:** Stability AI, RunwayML

## ğŸ“§ Contact

- **Author:** Amitabh Das
- **Email:** das.am@northeastern.edu
- **Institution:** Northeastern University, Khoury College of Computer Sciences

---

<p align="center">Made with â¤ï¸ for adversarial machine learning research</p>
