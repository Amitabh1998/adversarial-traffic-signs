# ğŸš¦ Adversarial Attacks on Traffic Sign Recognition Using Diffusion Models

[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-ee4c2c.svg)](https://pytorch.org/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

This repository contains the implementation for **"Adversarial Attacks on Traffic Sign Recognition Using Diffusion Models"**, a research project investigating the vulnerability of traffic sign classification systems to realistic, semantic adversarial perturbations generated by Stable Diffusion.

## ğŸ“‹ Overview

Traditional adversarial attacks (FGSM, PGD, C&W) create imperceptible pixel-level noise that can fool deep neural networks. However, these perturbations are easily detectable and don't represent realistic threat scenarios. This project explores **diffusion-based adversarial attacks** that generate realistic weather and lighting conditions (rain, fog, snow, glare) to create more naturalistic adversarial examples.

### Key Contributions

- ğŸŒ§ï¸ **Weather-based adversarial attacks** using Stable Diffusion img2img pipeline
- ğŸ“Š **Comprehensive benchmark** comparing diffusion vs. classical attacks (FGSM, PGD, C&W)
- ğŸ”„ **Transferability analysis** across multiple architectures (ResNet-50, EfficientNet-B0, ViT)
- ğŸ“ **Imperceptibility metrics** (LPIPS, SSIM) for attack quality assessment

## ğŸ—ï¸ Project Structure

```
adversarial-traffic-signs/
â”œâ”€â”€ configs/
â”‚   â””â”€â”€ config.yaml          # Configuration file
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ config.py            # Configuration management
â”‚   â””â”€â”€ trainer.py           # Model training utilities
â”œâ”€â”€ data/
â”‚   â””â”€â”€ dataloader.py        # GTSRB dataset loaders
â”œâ”€â”€ models/
â”‚   â””â”€â”€ factory.py           # Model architectures (ResNet, EfficientNet, ViT)
â”œâ”€â”€ attacks/
â”‚   â”œâ”€â”€ classical.py         # FGSM, PGD, C&W attacks
â”‚   â””â”€â”€ diffusion.py         # Stable Diffusion attacks
â”œâ”€â”€ evaluation/
â”‚   â”œâ”€â”€ metrics.py           # LPIPS, SSIM, transferability
â”‚   â””â”€â”€ visualization.py     # Result plotting
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ train.py             # Training script
â”‚   â”œâ”€â”€ evaluate_attacks.py  # Attack evaluation
â”‚   â””â”€â”€ generate_diffusion_attacks.py
â”œâ”€â”€ notebooks/               # Jupyter notebooks
â”œâ”€â”€ results/                 # Output directory
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ setup.py
â””â”€â”€ README.md
```

## ğŸš€ Quick Start

### 1. Installation

```bash
# Clone the repository
git clone https://github.com/amitabhdas/adversarial-traffic-signs.git
cd adversarial-traffic-signs

# Create virtual environment
python -m venv venv
source venv/bin/activate  # Linux/Mac
# or
.\venv\Scripts\activate  # Windows

# Install dependencies
pip install -r requirements.txt

# Or install as package
pip install -e ".[all]"
```

### 2. Download GTSRB Dataset

Download the German Traffic Sign Recognition Benchmark (GTSRB) from:
- [Official GTSRB Website](https://benchmark.ini.rub.de/gtsrb_dataset.html)
- [Kaggle Mirror](https://www.kaggle.com/datasets/meowmeowmeowmeowmeow/gtsrb-german-traffic-sign)

Extract to `data/GTSRB/` with the following structure:
```
data/GTSRB/
â”œâ”€â”€ GTSRB_final_training_images/
â”‚   â”œâ”€â”€ 00000/
â”‚   â”œâ”€â”€ 00001/
â”‚   â””â”€â”€ ...
â”œâ”€â”€ GTSRB_final_test_images/
â””â”€â”€ GTSRB_Final_Test_GT/
    â””â”€â”€ GT-final_test.csv
```

### 3. Train Models

```bash
# Train all models (ResNet-50, EfficientNet-B0, ViT)
python scripts/train.py \
    --data_dir data/GTSRB \
    --models resnet50 efficientnet_b0 vit \
    --epochs 10 \
    --batch_size 64

# Train specific model
python scripts/train.py \
    --data_dir data/GTSRB \
    --models resnet50 \
    --epochs 20
```

### 4. Generate Diffusion Attacks

```bash
# Generate diffusion-based adversarial examples
python scripts/generate_diffusion_attacks.py \
    --data_dir data/GTSRB \
    --output_dir results/diffused_images \
    --num_samples 1000 \
    --strength 0.3
```

**Note:** Requires HuggingFace authentication for Stable Diffusion:
```python
from huggingface_hub import login
login("your_token_here")
```

### 5. Evaluate Attacks

```bash
# Evaluate all attacks
python scripts/evaluate_attacks.py \
    --data_dir data/GTSRB \
    --checkpoint_dir results/checkpoints \
    --diffusion_metadata results/diffused_images/metadata.csv \
    --output_dir results
```

## ğŸ“Š Results

### Model Performance (Clean Accuracy)

| Model           | Validation Acc | Test Acc |
|-----------------|----------------|----------|
| ResNet-50       | 99.92%         | ~98%     |
| EfficientNet-B0 | 99.87%         | ~97%     |
| ViT             | 97.76%         | ~97%     |

### Attack Success Rate Comparison

| Attack        | ResNet-50 | EfficientNet-B0 | ViT   | Avg ASR |
|---------------|-----------|-----------------|-------|---------|
| FGSM          | 57.1%     | 51.5%           | 47.3% | 52.0%   |
| PGD           | 64.6%     | 52.3%           | 47.5% | 54.8%   |
| C&W           | 59.6%     | 48.9%           | 45.1% | 51.2%   |
| **Diffusion** | **56.2%** | **57.3%**       | **57.2%** | **56.9%** |

### Imperceptibility Metrics

| Attack        | LPIPS (â†“) | SSIM (â†‘) |
|---------------|-----------|----------|
| FGSM          | 0.573     | 0.565    |
| PGD           | 0.573     | 0.565    |
| C&W           | 0.380     | 0.715    |
| **Diffusion** | **0.454** | **0.673** |

### Comprehensive Summary

| Attack        | Avg ASR (%) | LPIPS (â†“) | SSIM (â†‘) | All Models Fooled (%) |
|---------------|-------------|-----------|----------|----------------------|
| FGSM          | 51.97       | 0.573     | 0.565    | 42.50                |
| PGD           | 54.79       | 0.573     | 0.565    | 42.94                |
| C&W           | 51.20       | 0.380     | 0.715    | 42.13                |
| **Diffusion** | **56.90**   | **0.454** | **0.673**| **43.20**            |

### Transferability Analysis

Cross-model attack transfer rates show diffusion attacks achieve consistent performance across architectures:

| Source Model    | EfficientNet-B0 | ViT   | ResNet-50 |
|-----------------|-----------------|-------|-----------|
| ResNet-50       | 53.3%           | 45.1% | -         |
| EfficientNet-B0 | -               | 46.3% | 53.3%     |
| ViT             | 46.3%           | -     | 45.1%     |

### Key Insights

1. **Diffusion attacks achieve highest average attack success rate (56.9%)** while maintaining consistent transferability across all three architectures
2. **C&W attacks have lowest LPIPS (0.380)** indicating minimal perceptual difference, but require expensive optimization
3. **PGD achieves highest ASR on ResNet-50 (64.6%)** but shows architecture-specific vulnerability patterns
4. **Diffusion attacks uniquely maintain uniform effectiveness** across CNN and Transformer architectures (56-57% ASR on all models)
5. **Weather-based perturbations are semantically meaningful** - they represent realistic environmental conditions rather than pixel-level noise

## ğŸ”§ Configuration

Edit `configs/config.yaml` to customize:

```yaml
# Training settings
training:
  batch_size: 64
  num_epochs: 10
  learning_rate: 0.001

# Attack parameters
attacks:
  fgsm:
    eps: 0.03
  pgd:
    eps: 0.03
    alpha: 0.01
    steps: 10
  cw:
    c: 1
    steps: 100

# Diffusion settings
diffusion:
  model: "runwayml/stable-diffusion-v1-5"
  strength: 0.3
  guidance_scale: 7.5
```

## ğŸ“ Using with Google Colab

For GPU access, use Google Colab:

```python
# Mount Drive
from google.colab import drive
drive.mount('/content/drive')

# Clone repo
!git clone https://github.com/amitabhdas/adversarial-traffic-signs.git
%cd adversarial-traffic-signs

# Install dependencies
!pip install -r requirements.txt

# Run training
!python scripts/train.py --data_dir /content/GTSRB --epochs 10
```

## ğŸ§ª API Usage

```python
from models.factory import get_model, load_model
from attacks.classical import ClassicalAttacks
from attacks.diffusion import DiffusionAttacker
from evaluation.metrics import MetricsCalculator

# Load model
model = load_model('resnet50', 'checkpoints/resnet50_best.pth')

# Classical attacks
attacker = ClassicalAttacks(model, fgsm_eps=0.03)
adv_images = attacker.generate('FGSM', images, labels)

# Diffusion attacks
diff_attacker = DiffusionAttacker()
adv_image, prompt = diff_attacker.generate_single(image)

# Evaluate metrics
metrics = MetricsCalculator()
lpips_score = metrics.compute_lpips(original, adversarial)
ssim_score = metrics.compute_ssim(original, adversarial)
```

## ğŸ“– Citation

If you use this code in your research, please cite:

```bibtex
@misc{das2025adversarial,
  author = {Das, Amitabh},
  title = {Adversarial Attacks on Traffic Sign Recognition Using Diffusion Models},
  year = {2025},
  institution = {Northeastern University},
  advisor = {Elhamifar, Ehsan}
}
```

## ğŸ“œ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- **Advisor:** Prof. Ehsan Elhamifar, Northeastern University
- **Dataset:** German Traffic Sign Recognition Benchmark (GTSRB)
- **Models:** HuggingFace Transformers, timm
- **Diffusion:** Stability AI, RunwayML

## ğŸ“§ Contact

- **Author:** Amitabh Das
- **Email:** das.am@northeastern.edu
- **Institution:** Northeastern University, Khoury College of Computer Sciences

---

<p align="center">Made with â¤ï¸ for adversarial machine learning research</p>
